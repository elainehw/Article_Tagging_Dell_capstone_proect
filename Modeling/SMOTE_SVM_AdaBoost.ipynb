{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import basic packages for managing dataframe\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# import visualization packages\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "\n",
    "# regardless warnings\n",
    "import warnings \n",
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# import model package \n",
    "# regression and classification\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "#clustering\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# import measurement package\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, f1_score\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "# PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#for handling imbalanced data \n",
    "import imblearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run preproccesing pipeline or bring in data\n",
    "#### ---> brought in training data from Microsoft Teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bring in data \n",
    "df_train = pd.read_csv('train.csv',engine='python')\n",
    "df_test = pd.read_csv('test.csv',engine='python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove those row with tags not included in top 200 tag list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_no_tag(df):\n",
    "    mask = (df.Tags=='[]') #create vector with 1 element for each row of the df (either a 1 or a 0)\n",
    "    df_new = df[~mask] #give me all the rows where this mask conditiion does not occur\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_new = remove_no_tag(df_train)\n",
    "df_test_new = remove_no_tag(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1627"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shape Y and X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_not_include = ['RECORDID','Tags','Tag_Count' 'Vec_Tags', 'Body', 'Body_Lemm',\n",
    "       'Body_Lemm_lower', 'Body_Token','Body_Length','Title', 'Title_Lemm',\n",
    "       'Title_Lemm_lower', 'Title_Token']\n",
    "\n",
    "# split train and test\n",
    "X_train = df_train_new.loc[:, ~df_train_new.columns.isin(columns_not_include)] #new bitwise mask \n",
    "\n",
    "X_test = df_test_new.loc[:, ~df_test_new.columns.isin(columns_not_include)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train= X_train.iloc[:,2:]\n",
    "X_test= X_test.iloc[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run PCA to reduce dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans_pca(df, variance):\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    # fit on training set only.\n",
    "    scaler.fit(df)\n",
    "    # apply transform to both the training set and the test set.\n",
    "    df = scaler.transform(df)\n",
    "\n",
    "    # variance = 0.9 indicates retaining 90% of total variance\n",
    "    pca = PCA(variance)\n",
    "    pca.fit(df)\n",
    "    # print(pca.n_components_)\n",
    "    df = pca.transform(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PCA for the total dataset(Train+Test)\n",
    "\n",
    "#X_total = pd.concat([X_train, X_test])\n",
    "X_train = X_train.fillna(0)\n",
    "X_test = X_test.fillna(0)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "pca = PCA(0.9)\n",
    "pca.fit(X_train)\n",
    "X_train = pd.DataFrame(pca.transform(X_train))\n",
    "X_test = pd.DataFrame(pca.transform(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM + Tomek Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change from dataframe to numpy array to use in tomek links sampler\n",
    "X_train_arr= np.array(X_train)\n",
    "#print(X_train_arr)\n",
    "\n",
    "X_test_arr= np.array(X_test)\n",
    "#print(X_test_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy_svm_tl = []\n",
    "#f1_score_svm_tl = []\n",
    "#roc_svm_tl = []\n",
    "\n",
    "# compute how many 1 in each model\n",
    "sum_train=[]\n",
    "sum_test=[]\n",
    "\n",
    "# Try first 10 models\n",
    "for num_models in range(0,10): \n",
    "    # create train and test\n",
    "    y_train = []\n",
    "    y_test = []\n",
    "    for i  in range(len(df_train_new)):\n",
    "        y_train.append(int(df_train_new['Vec_Tags'].iloc[i].split(']')[0].split('[')[1].split(',')[num_models].rstrip(' ').lstrip(' ')))\n",
    "    for j  in range(len(df_test_new)):\n",
    "        y_test.append(int(df_test_new['Vec_Tags'].iloc[j].split(']')[0].split('[')[1].split(',')[num_models].rstrip(' ').lstrip(' ')))\n",
    "    \n",
    "    sum_train.append(sum(y_train))\n",
    "    sum_test.append(sum(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_arr= np.array(y_train)\n",
    "y_test_arr= np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tomek Links- Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before UnderSampling, the shape of train_X: (1627, 358)\n",
      "Before UnderSampling, the shape of train_y: (1627,) \n",
      "\n",
      "Before UnderSampling, counts of label '1': 31\n",
      "Before UnderSampling, counts of label '0': 1596 \n",
      "\n",
      "After UnderSampling, the shape of train_X: (1627, 358)\n",
      "After UnderSampling, the shape of train_y: (1627,) \n",
      "\n",
      "After UnderSampling, counts of label '1': 31\n",
      "After UnderSampling, counts of label '0': 1596\n"
     ]
    }
   ],
   "source": [
    "#Tomek Links-- sampling_strategy= 'not minority'\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "print('Before UnderSampling, the shape of train_X: {}'.format(X_train_arr.shape)) \n",
    "print('Before UnderSampling, the shape of train_y: {} \\n'.format(y_train_arr.shape)) \n",
    "\n",
    "print(\"Before UnderSampling, counts of label '1': {}\".format(sum(y_train_arr == 1))) \n",
    "print(\"Before UnderSampling, counts of label '0': {} \\n\".format(sum(y_train_arr == 0)))\n",
    "\n",
    "tl= TomekLinks(sampling_strategy='not minority')\n",
    "X_train_uns, y_train_uns = tl.fit_resample(X_train_arr, y_train_arr) #uns= denotes undersampled data\n",
    "\n",
    "print('After UnderSampling, the shape of train_X: {}'.format(X_train_uns.shape)) \n",
    "print('After UnderSampling, the shape of train_y: {} \\n'.format(y_train_uns.shape)) \n",
    "  \n",
    "print(\"After UnderSampling, counts of label '1': {}\".format(sum(y_train_uns == 1))) \n",
    "print(\"After UnderSampling, counts of label '0': {}\".format(sum(y_train_uns == 0))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before UnderSampling, the shape of train_X: (1627, 358)\n",
      "Before UnderSampling, the shape of train_y: (1627,) \n",
      "\n",
      "Before UnderSampling, counts of label '1': 31\n",
      "Before UnderSampling, counts of label '0': 1596 \n",
      "\n",
      "After UnderSampling, the shape of train_X: (1627, 358)\n",
      "After UnderSampling, the shape of train_y: (1627,) \n",
      "\n",
      "After UnderSampling, counts of label '1': 31\n",
      "After UnderSampling, counts of label '0': 1596\n"
     ]
    }
   ],
   "source": [
    "#Tomek Links-- sampling_strategy= 'not majority'\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "print('Before UnderSampling, the shape of train_X: {}'.format(X_train_arr.shape)) \n",
    "print('Before UnderSampling, the shape of train_y: {} \\n'.format(y_train_arr.shape)) \n",
    "\n",
    "print(\"Before UnderSampling, counts of label '1': {}\".format(sum(y_train_arr == 1))) \n",
    "print(\"Before UnderSampling, counts of label '0': {} \\n\".format(sum(y_train_arr == 0)))\n",
    "\n",
    "tl= TomekLinks(sampling_strategy='not majority')\n",
    "X_train_uns, y_train_uns = tl.fit_resample(X_train_arr, y_train_arr) #uns= denotes undersampled data\n",
    "\n",
    "print('After UnderSampling, the shape of train_X: {}'.format(X_train_uns.shape)) \n",
    "print('After UnderSampling, the shape of train_y: {} \\n'.format(y_train_uns.shape)) \n",
    "  \n",
    "print(\"After UnderSampling, counts of label '1': {}\".format(sum(y_train_uns == 1))) \n",
    "print(\"After UnderSampling, counts of label '0': {}\".format(sum(y_train_uns == 0))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lmao it literally made no difference, even when 'not minority'  and 'not majority' sampling strategy is specified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVM and SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate X_train and X_test again\n",
    "X_train = df_train_new.loc[:, ~df_train_new.columns.isin(columns_not_include)]\n",
    "\n",
    "X_test = df_test_new.loc[:, ~df_test_new.columns.isin(columns_not_include)]\n",
    "\n",
    "\n",
    "X_train= X_train.iloc[:,2:]\n",
    "X_test= X_test.iloc[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PCA for the total dataset(Train+Test)\n",
    "\n",
    "X_train = X_train.fillna(0)\n",
    "X_test = X_test.fillna(0)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "pca = PCA(0.9)\n",
    "pca.fit(X_train)\n",
    "X_train = pd.DataFrame(pca.transform(X_train))\n",
    "X_test = pd.DataFrame(pca.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change from dataframe to numpy array to use in smote\n",
    "X_train_arr= np.array(X_train)\n",
    "#print(X_train_arr)\n",
    "\n",
    "X_test_arr= np.array(X_test)\n",
    "#print(X_test_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE \n",
    "\n",
    "accuracy_svm_sm = [] #sm- denotes using smote data set\n",
    "f1_score_svm_sm = []\n",
    "roc_svm_sm = []\n",
    "\n",
    "# compute how many 1 in each model\n",
    "sum_train=[]\n",
    "sum_test=[]\n",
    "\n",
    "# Try first 10 models\n",
    "for num_models in range(0,10): \n",
    "    # create train and test\n",
    "    y_train = []\n",
    "    y_test = []\n",
    "    for i  in range(len(df_train_new)):\n",
    "        y_train.append(int(df_train_new['Vec_Tags'].iloc[i].split(']')[0].split('[')[1].split(',')[num_models].rstrip(' ').lstrip(' ')))\n",
    "    for j  in range(len(df_test_new)):\n",
    "        y_test.append(int(df_test_new['Vec_Tags'].iloc[j].split(']')[0].split('[')[1].split(',')[num_models].rstrip(' ').lstrip(' ')))\n",
    "    \n",
    "    sum_train.append(sum(y_train))\n",
    "    sum_test.append(sum(y_test))\n",
    "    \n",
    "    y_train_arr= np.array(y_train)\n",
    "    y_test_arr= np.array(y_test)\n",
    "    \n",
    "    #SMOTE\n",
    "    sm = SMOTE(random_state = 2) \n",
    "    X_train_ovs, y_train_ovs = sm.fit_sample(X_train_arr, y_train_arr.ravel()) #ovs= denotes oversampled data\n",
    "    \n",
    "    #Linear SVM\n",
    "    svm_sm = LinearSVC().fit(X_train_ovs, y_train_ovs)\n",
    "    y_pred_svm_sm_test = svm_sm.predict(X_test)\n",
    "    y_pred_svm_sm_train = svm_sm.predict(X_train_ovs)\n",
    "\n",
    "    #y_score_svm = svm.predict_proba(X_test)[:,1]\n",
    "    acc_svm_sm = accuracy_score(y_test, y_pred_svm_sm_test)\n",
    "    f1_score_svm_sm_result = metrics.f1_score(y_test, y_pred_svm_sm_test)\n",
    "\n",
    "    #roc_svm_result = metrics.roc_auc_score(y_test, y_score_svm)\n",
    "    accuracy_svm_sm.append(acc_svm_sm)\n",
    "    f1_score_svm_sm.append(f1_score_svm_sm_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>sum_train</th>\n",
       "      <th>sum_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.970350</td>\n",
       "      <td>0.819672</td>\n",
       "      <td>0.044173</td>\n",
       "      <td>0.067385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.973046</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.031642</td>\n",
       "      <td>0.061995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.964960</td>\n",
       "      <td>0.763636</td>\n",
       "      <td>0.031328</td>\n",
       "      <td>0.059299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.948787</td>\n",
       "      <td>0.732394</td>\n",
       "      <td>0.030702</td>\n",
       "      <td>0.094340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.948787</td>\n",
       "      <td>0.732394</td>\n",
       "      <td>0.030702</td>\n",
       "      <td>0.094340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.981132</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>0.020363</td>\n",
       "      <td>0.035040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.975741</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.015977</td>\n",
       "      <td>0.029650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.986523</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.014411</td>\n",
       "      <td>0.021563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.894879</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.010025</td>\n",
       "      <td>0.016173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.997305</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.009712</td>\n",
       "      <td>0.024259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  f1_score  sum_train  sum_test\n",
       "0  0.970350  0.819672   0.044173  0.067385\n",
       "1  0.973046  0.821429   0.031642  0.061995\n",
       "2  0.964960  0.763636   0.031328  0.059299\n",
       "3  0.948787  0.732394   0.030702  0.094340\n",
       "4  0.948787  0.732394   0.030702  0.094340\n",
       "5  0.981132  0.758621   0.020363  0.035040\n",
       "6  0.975741  0.689655   0.015977  0.029650\n",
       "7  0.986523  0.666667   0.014411  0.021563\n",
       "8  0.894879  0.133333   0.010025  0.016173\n",
       "9  0.997305  0.947368   0.009712  0.024259"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tuples = list(zip(accuracy_svm_sm,f1_score_svm_sm,sum_train,sum_test))\n",
    "result_svm_sm = pd.DataFrame(data_tuples,columns=['accuracy','f1_score','sum_train','sum_test'])\n",
    "result_svm_sm['sum_train'] = result_svm_sm['sum_train']/len(y_train_ovs)\n",
    "result_svm_sm['sum_test'] = result_svm_sm['sum_test']/len(y_test)\n",
    "result_svm_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.706517008172844"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " result_svm_sm[\"f1_score\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "### Isolating this step to show what Smote does"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Smote- Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before UnderSampling, the shape of train_X: (1627, 358)\n",
      "Before UnderSampling, the shape of train_y: (1627,) \n",
      "\n",
      "Before OverSampling, counts of label '1': 31\n",
      "Before OverSampling, counts of label '0': 1596 \n",
      "\n",
      "After OverSampling, the shape of train_X: (3192, 358)\n",
      "After OverSampling, the shape of train_y: (3192,) \n",
      "\n",
      "After OverSampling, counts of label '1': 1596\n",
      "After OverSampling, counts of label '0': 1596\n"
     ]
    }
   ],
   "source": [
    "#Smote\n",
    "print('Before UnderSampling, the shape of train_X: {}'.format(X_train_arr.shape)) \n",
    "print('Before UnderSampling, the shape of train_y: {} \\n'.format(y_train_arr.shape)) \n",
    "\n",
    "print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train_arr == 1))) \n",
    "print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train_arr == 0))) \n",
    "  \n",
    "# import SMOTE module from imblearn library \n",
    "# pip install imblearn (if you don't have imblearn in your system) \n",
    "from imblearn.over_sampling import SMOTE \n",
    "sm = SMOTE(random_state = 2) \n",
    "X_train_ovs, y_train_ovs = sm.fit_sample(X_train_arr, y_train_arr.ravel()) #ovs= denotes oversampled data\n",
    "  \n",
    "print('After OverSampling, the shape of train_X: {}'.format(X_train_ovs.shape)) \n",
    "print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_ovs.shape)) \n",
    "  \n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_ovs == 1))) \n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_ovs == 0))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE and AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate X_train and X_test again\n",
    "X_train = df_train_new.loc[:, ~df_train_new.columns.isin(columns_not_include)]\n",
    "\n",
    "X_test = df_test_new.loc[:, ~df_test_new.columns.isin(columns_not_include)]\n",
    "\n",
    "\n",
    "X_train= X_train.iloc[:,2:]\n",
    "X_test= X_test.iloc[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PCA for the total dataset(Train+Test)\n",
    "\n",
    "X_train = X_train.fillna(0)\n",
    "X_test = X_test.fillna(0)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "pca = PCA(0.9)\n",
    "pca.fit(X_train)\n",
    "X_train = pd.DataFrame(pca.transform(X_train))\n",
    "X_test = pd.DataFrame(pca.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change from dataframe to numpy array to use in smote\n",
    "X_train_arr= np.array(X_train)\n",
    "X_test_arr= np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prettywrite(l,filename):\n",
    "    with open(filename, \"a+\") as f:\n",
    "        f.write(prettyprint(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost with using SVM model as base estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "accuracy_abc_ovs = []\n",
    "f1_score_abc_ovs = []\n",
    "roc_abc_ovs = []\n",
    "\n",
    "# compute how many 1 in each model\n",
    "sum_train=[]\n",
    "sum_test=[]\n",
    "\n",
    "# Try first 10 models\n",
    "for num_models in range(0,10): \n",
    "    # create train and test\n",
    "    y_train = []\n",
    "    y_test = []\n",
    "    for i  in range(len(df_train_new)):\n",
    "        y_train.append(int(df_train_new['Vec_Tags'].iloc[i].split(']')[0].split('[')[1].split(',')[num_models].rstrip(' ').lstrip(' ')))\n",
    "    for j  in range(len(df_test_new)):\n",
    "        y_test.append(int(df_test_new['Vec_Tags'].iloc[j].split(']')[0].split('[')[1].split(',')[num_models].rstrip(' ').lstrip(' ')))\n",
    "    \n",
    "    sum_train.append(sum(y_train))\n",
    "    sum_test.append(sum(y_test))\n",
    "    \n",
    "    y_train_arr= np.array(y_train)\n",
    "    y_test_arr= np.array(y_test)\n",
    "    \n",
    "    # import SMOTE module from imblearn library \n",
    "    # pip install imblearn (if you don't have imblearn in your system) \n",
    "    from imblearn.over_sampling import SMOTE \n",
    "    sm = SMOTE(random_state = 2) \n",
    "    X_train_ovs, y_train_ovs = sm.fit_sample(X_train_arr, y_train_arr.ravel()) #ovs= denotes oversampled data\n",
    "    \n",
    "    \n",
    "    #Adaboost\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    \n",
    "    #use svm_sm (the previous model) as the base estimator \n",
    "\n",
    "    # Create adaboost classifer object\n",
    "    abc =AdaBoostClassifier(n_estimators=50, base_estimator=svm_sm, algorithm='SAMME', learning_rate=1)\n",
    "\n",
    "    # Train Adaboost Classifer with oversampled data\n",
    "    abc_ovs = abc.fit(X_train_ovs, y_train_ovs)\n",
    "    y_pred_abc_ovs_test = abc_ovs.predict(X_test)\n",
    "    y_pred_abc_ovs_train = abc_ovs.predict(X_train_ovs)\n",
    "\n",
    "    #y_score_ab_ovs = svm.predict_proba(X_test)[:,1]\n",
    "    acc_abc_ovs = accuracy_score(y_test, y_pred_abc_ovs_test)\n",
    "    f1_score_abc_ovs_result = metrics.f1_score(y_test, y_pred_abc_ovs_test)\n",
    "\n",
    "    #roc_svm_result = metrics.roc_auc_score(y_test, y_score_svm)\n",
    "    accuracy_abc_ovs.append(acc_abc_ovs)\n",
    "    f1_score_abc_ovs.append(f1_score_abc_ovs_result)\n",
    "    \n",
    "    l=[acc_abc_ovs,f1_score_abc_ovs_result]\n",
    "    \n",
    "    prettywrite(l,\"results.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>sum_train</th>\n",
       "      <th>sum_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.962264</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.044173</td>\n",
       "      <td>0.067385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.967655</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.031642</td>\n",
       "      <td>0.061995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.967655</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.031328</td>\n",
       "      <td>0.059299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.946092</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.030702</td>\n",
       "      <td>0.094340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.946092</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.030702</td>\n",
       "      <td>0.094340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.986523</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.020363</td>\n",
       "      <td>0.035040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.986523</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.015977</td>\n",
       "      <td>0.029650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.991914</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.014411</td>\n",
       "      <td>0.021563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.916442</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>0.010025</td>\n",
       "      <td>0.016173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.997305</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.009712</td>\n",
       "      <td>0.024259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  f1_score  sum_train  sum_test\n",
       "0  0.962264  0.781250   0.044173  0.067385\n",
       "1  0.967655  0.785714   0.031642  0.061995\n",
       "2  0.967655  0.777778   0.031328  0.059299\n",
       "3  0.946092  0.729730   0.030702  0.094340\n",
       "4  0.946092  0.729730   0.030702  0.094340\n",
       "5  0.986523  0.814815   0.020363  0.035040\n",
       "6  0.986523  0.800000   0.015977  0.029650\n",
       "7  0.991914  0.823529   0.014411  0.021563\n",
       "8  0.916442  0.205128   0.010025  0.016173\n",
       "9  0.997305  0.947368   0.009712  0.024259"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tuples = list(zip(accuracy_abc_ovs,f1_score_abc_ovs,sum_train,sum_test))\n",
    "result_abc_ovs = pd.DataFrame(data_tuples,columns=['accuracy','f1_score','sum_train','sum_test'])\n",
    "result_abc_ovs['sum_train'] = result_abc_ovs['sum_train']/len(y_train_ovs)\n",
    "result_abc_ovs['sum_test'] = result_abc_ovs['sum_test']/len(y_test)\n",
    "result_abc_ovs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.739504237571188"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " result_abc_ovs[\"f1_score\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost using default base estimator--- DecisionTreeClassifier(max_depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate X_train and X_test again\n",
    "X_train = df_train_new.loc[:, ~df_train_new.columns.isin(columns_not_include)]\n",
    "\n",
    "X_test = df_test_new.loc[:, ~df_test_new.columns.isin(columns_not_include)]\n",
    "\n",
    "\n",
    "X_train= X_train.iloc[:,2:]\n",
    "X_test= X_test.iloc[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PCA for the total dataset(Train+Test)\n",
    "\n",
    "X_train = X_train.fillna(0)\n",
    "X_test = X_test.fillna(0)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "pca = PCA(0.9)\n",
    "pca.fit(X_train)\n",
    "X_train = pd.DataFrame(pca.transform(X_train))\n",
    "X_test = pd.DataFrame(pca.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change from dataframe to numpy array to use in smote\n",
    "X_train_arr= np.array(X_train)\n",
    "X_test_arr= np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d for default\n",
    "accuracy_abc_ovs_d = []\n",
    "f1_score_abc_ovs_d = []\n",
    "roc_abc_ovs_d = []\n",
    "\n",
    "# compute how many 1 in each model\n",
    "sum_train=[]\n",
    "sum_test=[]\n",
    "\n",
    "# Try first 10 models\n",
    "for num_models in range(0,10): \n",
    "    # create train and test\n",
    "    y_train = []\n",
    "    y_test = []\n",
    "    for i  in range(len(df_train_new)):\n",
    "        y_train.append(int(df_train_new['Vec_Tags'].iloc[i].split(']')[0].split('[')[1].split(',')[num_models].rstrip(' ').lstrip(' ')))\n",
    "    for j  in range(len(df_test_new)):\n",
    "        y_test.append(int(df_test_new['Vec_Tags'].iloc[j].split(']')[0].split('[')[1].split(',')[num_models].rstrip(' ').lstrip(' ')))\n",
    "    \n",
    "    sum_train.append(sum(y_train))\n",
    "    sum_test.append(sum(y_test))\n",
    "    \n",
    "    y_train_arr= np.array(y_train)\n",
    "    y_test_arr= np.array(y_test)\n",
    "    \n",
    "    # import SMOTE module from imblearn library \n",
    "    # pip install imblearn (if you don't have imblearn in your system) \n",
    "    from imblearn.over_sampling import SMOTE \n",
    "    sm = SMOTE(random_state = 2) \n",
    "    X_train_ovs, y_train_ovs = sm.fit_sample(X_train_arr, y_train_arr.ravel()) #ovs= denotes oversampled data\n",
    "    \n",
    "    \n",
    "    #Adaboost\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "    # Create adaboost classifer object\n",
    "    abc_d =AdaBoostClassifier(n_estimators=50, learning_rate=1)\n",
    "\n",
    "    # Train Adaboost Classifer with oversampled data\n",
    "    abc_ovs_d = abc_d.fit(X_train_ovs, y_train_ovs)\n",
    "    y_pred_abc_ovs_d_test = abc_ovs_d.predict(X_test)\n",
    "    y_pred_abc_ovs_d_train = abc_ovs_d.predict(X_train_ovs)\n",
    "\n",
    "    #y_score_ab_ovs = svm.predict_proba(X_test)[:,1]\n",
    "    acc_abc_ovs_d = accuracy_score(y_test, y_pred_abc_ovs_d_test)\n",
    "    f1_score_abc_ovs_d_result = metrics.f1_score(y_test, y_pred_abc_ovs_d_test)\n",
    "\n",
    "    #roc_svm_result = metrics.roc_auc_score(y_test, y_score_svm)\n",
    "    accuracy_abc_ovs_d.append(acc_abc_ovs_d)\n",
    "    f1_score_abc_ovs_d.append(f1_score_abc_ovs_d_result)\n",
    "    \n",
    "    l=[acc_abc_ovs_d,f1_score_abc_ovs_d_result]\n",
    "    \n",
    "    prettywrite(l,\"results.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>sum_train</th>\n",
       "      <th>sum_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.986523</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.044173</td>\n",
       "      <td>0.067385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.986523</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.031642</td>\n",
       "      <td>0.061995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.978437</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.031328</td>\n",
       "      <td>0.059299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.973046</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.030702</td>\n",
       "      <td>0.094340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.973046</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.030702</td>\n",
       "      <td>0.094340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.994609</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.020363</td>\n",
       "      <td>0.035040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.989218</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.015977</td>\n",
       "      <td>0.029650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.986523</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.014411</td>\n",
       "      <td>0.021563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.970350</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.010025</td>\n",
       "      <td>0.016173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.991914</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.009712</td>\n",
       "      <td>0.024259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  f1_score  sum_train  sum_test\n",
       "0  0.986523  0.909091   0.044173  0.067385\n",
       "1  0.986523  0.883721   0.031642  0.061995\n",
       "2  0.978437  0.818182   0.031328  0.059299\n",
       "3  0.973046  0.848485   0.030702  0.094340\n",
       "4  0.973046  0.848485   0.030702  0.094340\n",
       "5  0.994609  0.928571   0.020363  0.035040\n",
       "6  0.989218  0.833333   0.015977  0.029650\n",
       "7  0.986523  0.666667   0.014411  0.021563\n",
       "8  0.970350  0.352941   0.010025  0.016173\n",
       "9  0.991914  0.842105   0.009712  0.024259"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tuples = list(zip(accuracy_abc_ovs_d,f1_score_abc_ovs_d,sum_train,sum_test))\n",
    "result_abc_ovs_d = pd.DataFrame(data_tuples,columns=['accuracy','f1_score','sum_train','sum_test'])\n",
    "result_abc_ovs_d['sum_train'] = result_abc_ovs_d['sum_train']/len(y_train_ovs)\n",
    "result_abc_ovs_d['sum_test'] = result_abc_ovs_d['sum_test']/len(y_test)\n",
    "result_abc_ovs_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7931581222674894"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " result_abc_ovs_d[\"f1_score\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretty Printer prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.844"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l= [0.091,0.992,.844]\n",
    "l[-1]\n",
    "#str(l)\n",
    "#brackets are just seen as characters in a string\n",
    "#pstr= \",\".join(map(str,l)) + \";\\n\" #map string converter across every element of l-- common is the joiner (value delimiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"adaboost_results.txt\", \"a+\") as f:\n",
    "    f.write(\"test bitch\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prettyprint(l):\n",
    "    pstr= \",\".join(map(str,l)) + \";\\n\"\n",
    "    return pstr\n",
    "\n",
    "def prettywrite(l,filename):\n",
    "    with open(filename, \"a+\") as f:\n",
    "        f.write(prettyprint(l))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.091,0.992,0.844;\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prettyprint(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "prettywrite(l,\"text.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
